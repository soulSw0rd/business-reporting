#!/usr/bin/env python3
"""
Data Pipeline Diagnostic & Repair
Diagnostic et correction du pipeline de donn√©es
Auteur: Crypto-Tracker Team
Date: 2025-01-08
Version: 1.0.0
"""

import sys
import os
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# Ajout du chemin pour les imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from PRODUCTION.core.config_manager import get_config

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataPipelineDiagnostic:
    """
    Diagnostic et r√©paration du pipeline de donn√©es
    """
    
    def __init__(self):
        """Initialisation du diagnostic"""
        self.config = get_config()
        self.data_config = get_config("data")
        self.processed_path = Path(self.data_config["processed_path"])
        self.issues_found = []
        self.repairs_made = []
        
        logger.info("DataPipelineDiagnostic initialis√©")
    
    def run_full_diagnostic(self) -> Dict[str, Any]:
        """
        Ex√©cute un diagnostic complet du pipeline de donn√©es
        
        RETOURNE:
        - Dict avec le rapport de diagnostic
        """
        logger.info("üîç D√©but du diagnostic complet du pipeline de donn√©es")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "status": "running",
            "checks": {},
            "issues": [],
            "repairs": [],
            "recommendations": []
        }
        
        # 1. V√©rification de l'existence des dossiers
        report["checks"]["directories"] = self._check_directories()
        
        # 2. V√©rification des fichiers de donn√©es
        report["checks"]["data_files"] = self._check_data_files()
        
        # 3. Validation de la structure des donn√©es
        report["checks"]["data_structure"] = self._validate_data_structure()
        
        # 4. V√©rification de l'int√©grit√© des donn√©es
        report["checks"]["data_integrity"] = self._check_data_integrity()
        
        # 5. Test de chargement des donn√©es
        report["checks"]["data_loading"] = self._test_data_loading()
        
        # 6. G√©n√©ration des recommandations
        report["recommendations"] = self._generate_recommendations()
        
        # 7. Application des corrections automatiques
        if self.issues_found:
            logger.info("üîß Application des corrections automatiques")
            self._apply_automatic_repairs()
            report["repairs"] = self.repairs_made
        
        report["status"] = "completed"
        report["issues"] = self.issues_found
        
        logger.info(f"‚úÖ Diagnostic termin√© - {len(self.issues_found)} probl√®mes d√©tect√©s")
        
        return report
    
    def _check_directories(self) -> Dict[str, Any]:
        """V√©rification de l'existence des dossiers requis"""
        logger.info("üìÅ V√©rification des dossiers")
        
        required_dirs = [
            "RESOURCES/data/processed",
            "RESOURCES/data/raw",
            "RESOURCES/data/training",
            "RESOURCES/data/exports",
            "RESOURCES/configs"
        ]
        
        result = {
            "status": "success",
            "existing_dirs": [],
            "missing_dirs": [],
            "created_dirs": []
        }
        
        for dir_path in required_dirs:
            path = Path(dir_path)
            if path.exists():
                result["existing_dirs"].append(str(path))
            else:
                result["missing_dirs"].append(str(path))
                # Cr√©er le dossier manquant
                path.mkdir(parents=True, exist_ok=True)
                result["created_dirs"].append(str(path))
                self.repairs_made.append(f"Dossier cr√©√©: {path}")
        
        if result["missing_dirs"]:
            result["status"] = "repaired"
        
        return result
    
    def _check_data_files(self) -> Dict[str, Any]:
        """V√©rification de l'existence des fichiers de donn√©es"""
        logger.info("üìÑ V√©rification des fichiers de donn√©es")
        
        expected_files = [
            "top_traders_extended.json",
            "top_traders_for_prediction.json",
            "market_data_extended.json",
            "sentiment_data.json",
            "historical_data.json"
        ]
        
        result = {
            "status": "success",
            "existing_files": [],
            "missing_files": [],
            "file_sizes": {}
        }
        
        for filename in expected_files:
            file_path = self.processed_path / filename
            if file_path.exists():
                result["existing_files"].append(filename)
                result["file_sizes"][filename] = file_path.stat().st_size
            else:
                result["missing_files"].append(filename)
                self.issues_found.append(f"Fichier manquant: {filename}")
        
        if result["missing_files"]:
            result["status"] = "issues_found"
        
        return result
    
    def _validate_data_structure(self) -> Dict[str, Any]:
        """Validation de la structure des donn√©es JSON"""
        logger.info("üîç Validation de la structure des donn√©es")
        
        result = {
            "status": "success",
            "valid_files": [],
            "invalid_files": [],
            "structure_issues": []
        }
        
        # Sch√©mas attendus
        schemas = {
            "top_traders_extended.json": {
                "type": "array",
                "required_fields": ["trader_id", "username", "total_pnl", "win_rate"]
            },
            "top_traders_for_prediction.json": {
                "type": "array",
                "required_fields": ["address", "pnl_7d", "pnl_30d"]
            },
            "market_data_extended.json": {
                "type": "object",
                "required_fields": ["cryptocurrencies"]
            },
            "sentiment_data.json": {
                "type": "object",
                "required_fields": ["overall_sentiment"]
            }
        }
        
        for filename, schema in schemas.items():
            file_path = self.processed_path / filename
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # Validation du type
                    if schema["type"] == "array" and not isinstance(data, list):
                        result["structure_issues"].append(f"{filename}: Attendu array, trouv√© {type(data).__name__}")
                        continue
                    elif schema["type"] == "object" and not isinstance(data, dict):
                        result["structure_issues"].append(f"{filename}: Attendu object, trouv√© {type(data).__name__}")
                        continue
                    
                    # Validation des champs requis
                    if schema["type"] == "array" and data:
                        sample = data[0]
                        for field in schema["required_fields"]:
                            if field not in sample:
                                result["structure_issues"].append(f"{filename}: Champ manquant '{field}'")
                    elif schema["type"] == "object":
                        for field in schema["required_fields"]:
                            if field not in data:
                                result["structure_issues"].append(f"{filename}: Champ manquant '{field}'")
                    
                    if filename not in [issue.split(":")[0] for issue in result["structure_issues"]]:
                        result["valid_files"].append(filename)
                        
                except json.JSONDecodeError as e:
                    result["invalid_files"].append(filename)
                    result["structure_issues"].append(f"{filename}: Erreur JSON - {str(e)}")
                except Exception as e:
                    result["invalid_files"].append(filename)
                    result["structure_issues"].append(f"{filename}: Erreur - {str(e)}")
        
        if result["structure_issues"]:
            result["status"] = "issues_found"
            self.issues_found.extend(result["structure_issues"])
        
        return result
    
    def _check_data_integrity(self) -> Dict[str, Any]:
        """V√©rification de l'int√©grit√© des donn√©es"""
        logger.info("üîí V√©rification de l'int√©grit√© des donn√©es")
        
        result = {
            "status": "success",
            "integrity_issues": [],
            "data_quality": {}
        }
        
        # V√©rification des traders
        traders_file = self.processed_path / "top_traders_extended.json"
        if traders_file.exists():
            try:
                with open(traders_file, 'r', encoding='utf-8') as f:
                    traders = json.load(f)
                
                if traders:
                    # V√©rification des valeurs nulles/manquantes
                    null_count = sum(1 for trader in traders if not trader.get('username'))
                    if null_count > 0:
                        result["integrity_issues"].append(f"Traders avec username manquant: {null_count}")
                    
                    # V√©rification des valeurs aberrantes
                    pnl_values = [trader.get('total_pnl', 0) for trader in traders if trader.get('total_pnl') is not None]
                    if pnl_values:
                        avg_pnl = sum(pnl_values) / len(pnl_values)
                        extreme_count = sum(1 for pnl in pnl_values if abs(pnl) > abs(avg_pnl) * 100)
                        if extreme_count > 0:
                            result["integrity_issues"].append(f"Valeurs PnL extr√™mes d√©tect√©es: {extreme_count}")
                    
                    result["data_quality"]["traders_count"] = len(traders)
                    result["data_quality"]["avg_pnl"] = avg_pnl if pnl_values else 0
                    
            except Exception as e:
                result["integrity_issues"].append(f"Erreur lecture traders: {str(e)}")
        
        # V√©rification des donn√©es de march√©
        market_file = self.processed_path / "market_data_extended.json"
        if market_file.exists():
            try:
                with open(market_file, 'r', encoding='utf-8') as f:
                    market_data = json.load(f)
                
                if 'cryptocurrencies' in market_data:
                    cryptos = market_data['cryptocurrencies']
                    result["data_quality"]["crypto_count"] = len(cryptos)
                    
                    # V√©rification des prix
                    prices = [crypto.get('price', 0) for crypto in cryptos if crypto.get('price')]
                    if prices:
                        zero_prices = sum(1 for price in prices if price <= 0)
                        if zero_prices > 0:
                            result["integrity_issues"].append(f"Cryptos avec prix <= 0: {zero_prices}")
                    
            except Exception as e:
                result["integrity_issues"].append(f"Erreur lecture march√©: {str(e)}")
        
        if result["integrity_issues"]:
            result["status"] = "issues_found"
            self.issues_found.extend(result["integrity_issues"])
        
        return result
    
    def _test_data_loading(self) -> Dict[str, Any]:
        """Test de chargement des donn√©es comme dans l'application"""
        logger.info("üß™ Test de chargement des donn√©es")
        
        result = {
            "status": "success",
            "loading_errors": [],
            "loaded_data": {}
        }
        
        try:
            # Simulation du chargement comme dans le dashboard
            scraped_data = {
                'traders_for_analysis': [],
                'traders_for_prediction': [],
                'market_data_extended': {},
                'sentiment_data': {},
                'historical_data': []
            }
            
            # Test chargement traders pour analyse
            traders_file = self.processed_path / "top_traders_extended.json"
            if traders_file.exists():
                with open(traders_file, 'r', encoding='utf-8') as f:
                    scraped_data['traders_for_analysis'] = json.load(f)
                result["loaded_data"]["traders_analysis"] = len(scraped_data['traders_for_analysis'])
            
            # Test chargement traders pour pr√©diction
            prediction_file = self.processed_path / "top_traders_for_prediction.json"
            if prediction_file.exists():
                with open(prediction_file, 'r', encoding='utf-8') as f:
                    scraped_data['traders_for_prediction'] = json.load(f)
                result["loaded_data"]["traders_prediction"] = len(scraped_data['traders_for_prediction'])
            
            # Test chargement donn√©es de march√©
            market_file = self.processed_path / "market_data_extended.json"
            if market_file.exists():
                with open(market_file, 'r', encoding='utf-8') as f:
                    scraped_data['market_data_extended'] = json.load(f)
                result["loaded_data"]["market_data"] = "loaded"
            
            # Test chargement sentiment
            sentiment_file = self.processed_path / "sentiment_data.json"
            if sentiment_file.exists():
                with open(sentiment_file, 'r', encoding='utf-8') as f:
                    scraped_data['sentiment_data'] = json.load(f)
                result["loaded_data"]["sentiment_data"] = "loaded"
            
            # Test chargement historique
            historical_file = self.processed_path / "historical_data.json"
            if historical_file.exists():
                with open(historical_file, 'r', encoding='utf-8') as f:
                    scraped_data['historical_data'] = json.load(f)
                result["loaded_data"]["historical_data"] = len(scraped_data['historical_data'])
            
        except Exception as e:
            result["loading_errors"].append(f"Erreur de chargement: {str(e)}")
            result["status"] = "error"
            self.issues_found.append(f"Erreur de chargement: {str(e)}")
        
        return result
    
    def _generate_recommendations(self) -> List[str]:
        """G√©n√©ration des recommandations d'am√©lioration"""
        recommendations = []
        
        if not (self.processed_path / "top_traders_extended.json").exists():
            recommendations.append("G√©n√©rer des donn√©es traders d'exemple")
        
        if not (self.processed_path / "market_data_extended.json").exists():
            recommendations.append("G√©n√©rer des donn√©es de march√© d'exemple")
        
        if not (self.processed_path / "sentiment_data.json").exists():
            recommendations.append("G√©n√©rer des donn√©es de sentiment d'exemple")
        
        if len(self.issues_found) > 5:
            recommendations.append("Consid√©rer une r√©g√©n√©ration compl√®te des donn√©es")
        
        recommendations.append("Mettre en place un syst√®me de validation automatique")
        recommendations.append("Impl√©menter un syst√®me de sauvegarde des donn√©es")
        
        return recommendations
    
    def _apply_automatic_repairs(self):
        """Application des corrections automatiques"""
        logger.info("üîß Application des corrections automatiques")
        
        # G√©n√©ration de donn√©es manquantes
        self._generate_missing_data()
        
        # Correction des formats de donn√©es
        self._fix_data_formats()
        
        # Nettoyage des donn√©es corrompues
        self._clean_corrupted_data()
    
    def _generate_missing_data(self):
        """G√©n√©ration des donn√©es manquantes"""
        
        # G√©n√©ration de donn√©es traders si manquantes
        if not (self.processed_path / "top_traders_extended.json").exists():
            self._generate_sample_traders()
        
        # G√©n√©ration de donn√©es de march√© si manquantes
        if not (self.processed_path / "market_data_extended.json").exists():
            self._generate_sample_market_data()
        
        # G√©n√©ration de donn√©es de sentiment si manquantes
        if not (self.processed_path / "sentiment_data.json").exists():
            self._generate_sample_sentiment_data()
    
    def _generate_sample_traders(self):
        """G√©n√©ration de donn√©es traders d'exemple"""
        import random
        
        traders = []
        for i in range(20):
            trader = {
                "trader_id": f"TopTrader_{i+1:03d}",
                "rank": i + 1,
                "username": f"trader_{i+1}",
                "total_pnl": random.uniform(-10000, 50000),
                "win_rate": random.uniform(0.4, 0.9),
                "total_trades": random.randint(50, 2000),
                "roi_percentage": random.uniform(-50, 200),
                "favorite_pairs": ["BTC/USDT", "ETH/USDT"],
                "country": "Unknown"
            }
            traders.append(trader)
        
        with open(self.processed_path / "top_traders_extended.json", 'w', encoding='utf-8') as f:
            json.dump(traders, f, indent=2, ensure_ascii=False)
        
        self.repairs_made.append("Donn√©es traders g√©n√©r√©es")
    
    def _generate_sample_market_data(self):
        """G√©n√©ration de donn√©es de march√© d'exemple"""
        market_data = {
            "cryptocurrencies": [
                {
                    "symbol": "BTC",
                    "price": 45000,
                    "change_24h": 2.5,
                    "volume_24h": 25000000000,
                    "market_cap": 850000000000
                },
                {
                    "symbol": "ETH",
                    "price": 2800,
                    "change_24h": 3.2,
                    "volume_24h": 15000000000,
                    "market_cap": 340000000000
                }
            ]
        }
        
        with open(self.processed_path / "market_data_extended.json", 'w', encoding='utf-8') as f:
            json.dump(market_data, f, indent=2, ensure_ascii=False)
        
        self.repairs_made.append("Donn√©es de march√© g√©n√©r√©es")
    
    def _generate_sample_sentiment_data(self):
        """G√©n√©ration de donn√©es de sentiment d'exemple"""
        import random
        
        sentiment_data = {
            "overall_sentiment": random.uniform(-1, 1),
            "social_sentiment": random.uniform(-1, 1),
            "news_sentiment": random.uniform(-1, 1),
            "signals": []
        }
        
        with open(self.processed_path / "sentiment_data.json", 'w', encoding='utf-8') as f:
            json.dump(sentiment_data, f, indent=2, ensure_ascii=False)
        
        self.repairs_made.append("Donn√©es de sentiment g√©n√©r√©es")
    
    def _fix_data_formats(self):
        """Correction des formats de donn√©es"""
        # Correction des formats num√©riques si n√©cessaire
        pass
    
    def _clean_corrupted_data(self):
        """Nettoyage des donn√©es corrompues"""
        # Suppression des entr√©es corrompues
        pass
    
    def save_report(self, report: Dict[str, Any], filename: str = None):
        """Sauvegarde du rapport de diagnostic"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"diagnostic_report_{timestamp}.json"
        
        report_path = Path("ADMIN/logs") / filename
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"üìä Rapport sauvegard√©: {report_path}")
        return report_path


def main():
    """Fonction principale"""
    diagnostic = DataPipelineDiagnostic()
    
    # Ex√©cution du diagnostic complet
    report = diagnostic.run_full_diagnostic()
    
    # Sauvegarde du rapport
    report_path = diagnostic.save_report(report)
    
    # Affichage du r√©sum√©
    print("\n" + "="*60)
    print("üîç RAPPORT DE DIAGNOSTIC DU PIPELINE DE DONN√âES")
    print("="*60)
    print(f"üìÖ Timestamp: {report['timestamp']}")
    print(f"üìä Statut: {report['status']}")
    print(f"‚ùå Probl√®mes d√©tect√©s: {len(report['issues'])}")
    print(f"üîß Corrections appliqu√©es: {len(report['repairs'])}")
    print(f"üí° Recommandations: {len(report['recommendations'])}")
    
    if report['issues']:
        print("\n‚ùå PROBL√àMES D√âTECT√âS:")
        for issue in report['issues']:
            print(f"   ‚Ä¢ {issue}")
    
    if report['repairs']:
        print("\nüîß CORRECTIONS APPLIQU√âES:")
        for repair in report['repairs']:
            print(f"   ‚Ä¢ {repair}")
    
    if report['recommendations']:
        print("\nüí° RECOMMANDATIONS:")
        for rec in report['recommendations']:
            print(f"   ‚Ä¢ {rec}")
    
    print(f"\nüìÑ Rapport d√©taill√©: {report_path}")
    print("="*60)


if __name__ == "__main__":
    main() 